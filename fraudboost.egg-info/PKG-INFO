Metadata-Version: 2.4
Name: fraudboost
Version: 0.1.0
Summary: A gradient boosting framework purpose-built for fraud detection in fintech
Home-page: https://github.com/AlexShrike/fraudboost
Author: Alex Shrike
Author-email: Alex Shrike <alex@example.com>
License: MIT License
        
        Copyright (c) 2026 Alex Shrike
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
Project-URL: Homepage, https://github.com/AlexShrike/fraudboost
Project-URL: Bug Tracker, https://github.com/AlexShrike/fraudboost/issues
Project-URL: Documentation, https://github.com/AlexShrike/fraudboost#readme
Project-URL: Source Code, https://github.com/AlexShrike/fraudboost
Keywords: machine-learning,fraud-detection,gradient-boosting,fintech,xgboost,anomaly-detection
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Financial and Insurance Industry
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Office/Business :: Financial
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=1.20.0
Requires-Dist: scipy>=1.7.0
Requires-Dist: pandas>=1.3.0
Requires-Dist: scikit-learn>=1.0.0
Requires-Dist: matplotlib>=3.4.0
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Provides-Extra: examples
Requires-Dist: xgboost>=1.6.0; extra == "examples"
Requires-Dist: jupyter; extra == "examples"
Requires-Dist: seaborn; extra == "examples"
Provides-Extra: full
Requires-Dist: pytest>=6.0; extra == "full"
Requires-Dist: pytest-cov; extra == "full"
Requires-Dist: xgboost>=1.6.0; extra == "full"
Requires-Dist: jupyter; extra == "full"
Requires-Dist: seaborn; extra == "full"
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# FraudBoost

**A gradient boosting framework purpose-built for fraud detection in fintech**

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![GitHub Issues](https://img.shields.io/github/issues/AlexShrike/fraudboost.svg)](https://github.com/AlexShrike/fraudboost/issues)

## What is FraudBoost?

FraudBoost is a novel gradient boosting algorithm specifically designed for fraud detection in financial technology. Unlike traditional gradient boosting methods that treat all errors equally, FraudBoost uses **value-weighted asymmetric loss functions** where:

- **False negatives** are weighted by transaction amount (missing a $10K fraud costs more than missing a $10 fraud)
- **False positives** have a fixed investigation cost (e.g., $100 per case reviewed)
- **Tree splits** are evaluated by net financial savings, not information gain

This results in models that optimize for **business impact** rather than just statistical accuracy.

## Key Features

üéØ **Value-Weighted Loss Functions**: Cost false negatives by transaction amount, false positives by investigation cost

üìä **Financial Impact Metrics**: Value Detection Rate (VDR), Net Savings, ROI, Cost-Benefit Analysis  

üåê **Spectral Graph Features**: Extract network patterns from transaction relationships (customer‚Üîmerchant, customer‚Üîdevice)

‚öñÔ∏è **Pareto-Optimal Thresholds**: Find optimal operating points across competing business objectives

üìà **Temporal Drift Detection**: Monitor model degradation and trigger retraining alerts

üîß **End-to-End Pipeline**: Complete workflow from feature extraction to production monitoring

## Why FraudBoost vs XGBoost?

| Feature | XGBoost | FraudBoost |
|---------|---------|-----------|
| **Loss Function** | Log-loss (all errors equal) | Value-weighted (FN ‚àù amount) |
| **Split Criterion** | Information gain | Net savings maximization |
| **Evaluation Metrics** | Accuracy, AUC, F1 | VDR, Net Savings, ROI |
| **Threshold Selection** | Manual/grid search | Pareto-optimal automation |
| **Graph Features** | Not supported | Built-in spectral extraction |
| **Drift Detection** | External tools required | Built-in PSI monitoring |
| **Business Focus** | General ML | Fraud detection specific |

## Installation

### From PyPI (recommended)
```bash
pip install fraudboost
```

### From Source
```bash
git clone https://github.com/AlexShrike/fraudboost.git
cd fraudboost
pip install -e .
```

### With Optional Dependencies
```bash
# For development and testing
pip install fraudboost[dev]

# For examples and benchmarks (includes XGBoost)
pip install fraudboost[examples]

# Everything
pip install fraudboost[full]
```

## Quick Start

### Basic Usage

```python
import numpy as np
from fraudboost import FraudBoostClassifier
from sklearn.model_selection import train_test_split

# Your fraud detection data
X, y = ...  # Features and labels (0=legit, 1=fraud) 
amounts = ...  # Transaction amounts

X_train, X_test, y_train, y_test, amounts_train, amounts_test = train_test_split(
    X, y, amounts, test_size=0.3, stratify=y
)

# Train FraudBoost with value weighting
model = FraudBoostClassifier(
    n_estimators=100,
    fp_cost=150,  # $150 cost per false positive investigation
    loss='value_weighted'  # Use value-weighted loss
)

model.fit(X_train, y_train, amounts=amounts_train)

# Make predictions
fraud_probabilities = model.predict_proba(X_test)[:, 1]
fraud_predictions = model.predict(X_test, threshold=0.3)  # Custom threshold
```

### Business Impact Evaluation

```python
from fraudboost import classification_report_fraud, value_detection_rate, net_savings

# Comprehensive fraud-specific evaluation
report = classification_report_fraud(y_test, fraud_predictions, amounts_test, fp_cost=150)
print(report)

# Key business metrics
vdr = value_detection_rate(y_test, fraud_predictions, amounts_test)
savings = net_savings(y_test, fraud_predictions, amounts_test, fp_cost=150)

print(f"Value Detection Rate: {vdr:.2%}")  # % of fraud dollars caught
print(f"Net Savings: ${savings:,.2f}")     # Profit after investigation costs
```

### Threshold Optimization

```python
from fraudboost import ParetoOptimizer

# Find optimal decision thresholds
optimizer = ParetoOptimizer(fp_cost=150)
optimizer.fit(y_test, fraud_probabilities, amounts_test)

# Get recommendation for maximizing savings
recommendation = optimizer.recommend_threshold('max_savings')
optimal_threshold = recommendation['threshold']
expected_savings = recommendation['metrics']['net_savings']

print(f"Optimal threshold: {optimal_threshold:.3f}")
print(f"Expected savings: ${expected_savings:,.2f}")

# Plot Pareto frontier
optimizer.plot_pareto_frontier('recall', 'precision')
```

### Complete Pipeline with Graph Features

```python
from fraudboost import FraudDetectionPipeline
import pandas as pd

# DataFrame with transaction relationships
df = pd.DataFrame({
    'feature_1': ...,
    'feature_2': ...,
    'customer_id': ...,    # Entity relationships
    'merchant_id': ...,    # for graph features
    'device_id': ...,
    'amount': ...,
    'is_fraud': ...
})

# Complete pipeline with spectral features
pipeline = FraudDetectionPipeline(
    use_spectral_features=True,  # Extract graph features
    optimize_threshold=True,     # Pareto optimization
    enable_drift_detection=True, # Monitor degradation
    fp_cost=150
)

# Fit end-to-end pipeline
X = df.drop(['amount', 'is_fraud'], axis=1)
y = df['is_fraud']
amounts = df['amount']

pipeline.fit(
    X, y, amounts,
    entity_columns=['customer_id', 'merchant_id', 'device_id']
)

# Comprehensive evaluation
evaluation = pipeline.evaluate(X_test, y_test, amounts_test)
print(f"Pipeline ROI: {evaluation['classification_metrics']['roi']:.2f}x")
```

## Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          FraudBoost Architecture                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Raw Transaction Data                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Features      ‚îÇ    ‚îÇ   Relationships   ‚îÇ    ‚îÇ   Amounts      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Amount        ‚îÇ    ‚îÇ ‚Ä¢ Customer ‚Üî Merch‚îÇ    ‚îÇ ‚Ä¢ $10 - $50K   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Time          ‚îÇ    ‚îÇ ‚Ä¢ Customer ‚Üî Device‚îÇ    ‚îÇ ‚Ä¢ Higher for   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Location      ‚îÇ    ‚îÇ ‚Ä¢ Merchant ‚Üî Device‚îÇ    ‚îÇ   fraud cases  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ ...           ‚îÇ    ‚îÇ ‚Ä¢ ...             ‚îÇ    ‚îÇ                ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ           ‚îÇ                        ‚îÇ                       ‚îÇ        ‚îÇ
‚îÇ           ‚ñº                        ‚ñº                       ‚ñº        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Feature         ‚îÇ    ‚îÇ Spectral Graph   ‚îÇ    ‚îÇ Value Weights   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Engineering     ‚îÇ    ‚îÇ Features         ‚îÇ    ‚îÇ                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ    ‚îÇ ‚Ä¢ Eigenvectors   ‚îÇ    ‚îÇ w_fn = amt/med  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ    ‚îÇ ‚Ä¢ Centrality     ‚îÇ    ‚îÇ w_fp = fp_cost  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ    ‚îÇ ‚Ä¢ PageRank       ‚îÇ    ‚îÇ                ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ           ‚îÇ                        ‚îÇ                       ‚îÇ        ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ        ‚îÇ
‚îÇ                        ‚ñº                                   ‚îÇ        ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ        ‚îÇ
‚îÇ              ‚îÇ Combined Feature ‚îÇ                          ‚îÇ        ‚îÇ
‚îÇ              ‚îÇ Matrix           ‚îÇ                          ‚îÇ        ‚îÇ
‚îÇ              ‚îÇ [Original + Graph]‚îÇ                          ‚îÇ        ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ        ‚îÇ
‚îÇ                        ‚îÇ                                   ‚îÇ        ‚îÇ
‚îÇ                        ‚ñº                                   ‚ñº        ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ              ‚îÇ  FraudBoost      ‚îÇ              ‚îÇ Value-Weighted  ‚îÇ   ‚îÇ
‚îÇ              ‚îÇ  Classifier      ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ Loss Function   ‚îÇ   ‚îÇ
‚îÇ              ‚îÇ                  ‚îÇ              ‚îÇ                 ‚îÇ   ‚îÇ
‚îÇ              ‚îÇ ‚Ä¢ Value-weighted ‚îÇ              ‚îÇ L = -[y¬∑w_fn¬∑   ‚îÇ   ‚îÇ
‚îÇ              ‚îÇ   tree splits    ‚îÇ              ‚îÇ     log(p) +    ‚îÇ   ‚îÇ
‚îÇ              ‚îÇ ‚Ä¢ Net savings    ‚îÇ              ‚îÇ  (1-y)¬∑w_fp¬∑    ‚îÇ   ‚îÇ
‚îÇ              ‚îÇ   optimization   ‚îÇ              ‚îÇ     log(1-p)]   ‚îÇ   ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                        ‚îÇ                                             ‚îÇ
‚îÇ                        ‚ñº                                             ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ
‚îÇ              ‚îÇ  Predictions     ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ  & Probabilities ‚îÇ                                    ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ
‚îÇ                        ‚îÇ                                             ‚îÇ
‚îÇ                        ‚ñº                                             ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ
‚îÇ              ‚îÇ Pareto Threshold ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ Optimization     ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ                  ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ ‚Ä¢ Max VDR        ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ ‚Ä¢ Max Savings    ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ ‚Ä¢ Min FPs        ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ ‚Ä¢ Balanced       ‚îÇ                                    ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ
‚îÇ                        ‚îÇ                                             ‚îÇ
‚îÇ                        ‚ñº                                             ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ
‚îÇ              ‚îÇ Business Impact  ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ Metrics          ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ                  ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ ‚Ä¢ Net Savings    ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ ‚Ä¢ ROI            ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ ‚Ä¢ VDR            ‚îÇ                                    ‚îÇ
‚îÇ              ‚îÇ ‚Ä¢ Cost-Benefit   ‚îÇ                                    ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Core Components

### 1. Value-Weighted Loss Functions

Traditional gradient boosting uses log-loss where all misclassifications have equal cost:

```
Standard Loss: L = -[y¬∑log(p) + (1-y)¬∑log(1-p)]
```

FraudBoost uses value-weighted asymmetric loss:

```
FraudBoost Loss: L = -[y¬∑w_fn(amount)¬∑log(p) + (1-y)¬∑w_fp¬∑log(1-p)]

where:
  w_fn = transaction_amount / median_amount  (higher for larger amounts)
  w_fp = fp_cost / median_amount             (fixed investigation cost)
```

### 2. Net Savings Split Criterion

Traditional decision trees maximize information gain. FraudBoost trees maximize **net savings**:

```
Net Savings Gain = (Fraud Caught Value) - (False Positive Cost)
                 = Œ£(amounts[TP]) - count(FP) √ó fp_cost
```

### 3. Spectral Graph Features

Extract network patterns from transaction relationships:

```python
# Build bipartite graph: customers ‚Üî merchants ‚Üî devices
G = create_transaction_graph(df, ['customer_id', 'merchant_id', 'device_id'])

# Compute Laplacian eigendecomposition
L = laplacian_matrix(G)
eigenvalues, eigenvectors = eigh(L)

# Extract node features:
# ‚Ä¢ Spectral energy (frequency bands)
# ‚Ä¢ Degree centrality
# ‚Ä¢ Local heterophily
# ‚Ä¢ PageRank
features = compute_node_features(eigenvectors, G)
```

### 4. Business Impact Metrics

Beyond accuracy, FraudBoost provides financial metrics:

- **Value Detection Rate (VDR)**: `Œ£(fraud_amounts_caught) / Œ£(all_fraud_amounts)`
- **Net Savings**: `fraud_prevented - investigation_costs`
- **ROI**: `fraud_prevented / investigation_costs`
- **Cost-Benefit Ratio**: `benefits / (investigation_cost + fraud_losses)`

## Examples

### Basic Classifier Example

Run the basic usage example:

```bash
cd examples/
python basic_usage.py
```

This demonstrates:
- Creating synthetic fraud data
- Training FraudBoost vs standard approaches
- Evaluating business impact metrics
- Threshold optimization

### Benchmark vs XGBoost

Compare FraudBoost against XGBoost:

```bash
cd examples/
python benchmark_vs_xgboost.py
```

Expected results on synthetic data:

| Metric | XGBoost | FraudBoost | Advantage |
|--------|---------|-----------|-----------|
| Precision | 0.756 | 0.798 | +0.042 |
| Recall | 0.623 | 0.634 | +0.011 |
| **VDR** | 0.651 | **0.742** | **+0.091** |
| **Net Savings** | $52,340 | **$67,890** | **+$15,550** |
| **ROI** | 4.2x | **5.8x** | **+1.6x** |

*Note: Results vary with data characteristics. FraudBoost excels when fraud amounts are significantly higher than legitimate transactions.*

## API Reference

### FraudBoostClassifier

```python
FraudBoostClassifier(
    n_estimators=100,        # Number of boosting rounds
    learning_rate=0.1,       # Shrinkage parameter
    max_depth=6,             # Maximum tree depth
    fp_cost=100.0,          # False positive investigation cost
    loss='value_weighted',   # Loss function type
    early_stopping_rounds=10 # Early stopping patience
)
```

**Methods:**
- `fit(X, y, amounts=None)`: Train the model
- `predict_proba(X)`: Get fraud probabilities
- `predict(X, threshold=0.5)`: Get binary predictions
- `get_feature_importance()`: Get feature importance scores

### Key Metrics Functions

```python
value_detection_rate(y_true, y_pred, amounts)
net_savings(y_true, y_pred, amounts, fp_cost=100)
roi(y_true, y_pred, amounts, fp_cost=100)
classification_report_fraud(y_true, y_pred, amounts, fp_cost=100)
```

### ParetoOptimizer

```python
optimizer = ParetoOptimizer(fp_cost=100)
optimizer.fit(y_true, y_pred_proba, amounts)

# Get optimal thresholds
recommendation = optimizer.recommend_threshold('max_savings')
pareto_points = optimizer.get_pareto_points()

# Visualize
optimizer.plot_pareto_frontier('recall', 'precision')
```

## Testing

Run the test suite:

```bash
# Install test dependencies
pip install pytest pytest-cov

# Run all tests
python -m pytest tests/ -v

# With coverage
python -m pytest tests/ -v --cov=fraudboost --cov-report=html
```

## Performance Considerations

### Computational Complexity

- **Training**: O(n √ó m √ó d √ó t) where n=samples, m=features, d=depth, t=trees
- **Prediction**: O(m √ó d √ó t) per sample
- **Spectral Features**: O(k¬≥ + e√ók) where k=nodes, e=edges

### Memory Usage

- **Core Algorithm**: ~O(n√óm) for feature matrix
- **Spectral Features**: ~O(k¬≤) for adjacency matrix (sparse storage used)
- **Tree Storage**: ~O(2^d √ó t) for all tree nodes

### Scalability Tips

```python
# For large datasets
model = FraudBoostClassifier(
    subsample=0.8,           # Sample 80% of rows per tree
    colsample_bytree=0.8,    # Sample 80% of features per tree
    max_depth=4,             # Shallower trees
    early_stopping_rounds=5  # Stop early
)

# For spectral features
extractor = SpectralFeatureExtractor(
    max_nodes=10000,         # Limit graph size
    min_degree=3             # Filter low-degree nodes
)
```

## Advanced Usage

### Custom Loss Functions

Implement your own loss function:

```python
from fraudboost.losses import BaseLoss

class CustomFraudLoss(BaseLoss):
    def __call__(self, y_true, y_pred, amounts=None):
        # Your loss computation
        pass
    
    def gradient(self, y_true, y_pred, amounts=None):
        # Gradient computation
        pass
    
    def hessian(self, y_true, y_pred, amounts=None):
        # Hessian computation  
        pass

# Use custom loss
model = FraudBoostClassifier(loss=CustomFraudLoss())
```

### Temporal Cross-Validation

Proper time-series validation for fraud detection:

```python
from sklearn.model_selection import TimeSeriesSplit

# Sort by timestamp
df_sorted = df.sort_values('timestamp')
X = df_sorted.drop(['is_fraud', 'amount'], axis=1).values
y = df_sorted['is_fraud'].values
amounts = df_sorted['amount'].values

# Time-based splits (no future leakage)
tscv = TimeSeriesSplit(n_splits=5)

for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    amounts_train, amounts_val = amounts[train_idx], amounts[val_idx]
    
    model.fit(X_train, y_train, amounts_train)
    predictions = model.predict_proba(X_val)[:, 1]
    
    # Evaluate on validation set
    vdr = value_detection_rate(y_val, predictions > 0.5, amounts_val)
    print(f"Validation VDR: {vdr:.4f}")
```

### Production Deployment

Monitor model performance in production:

```python
from fraudboost import TemporalDriftDetector

# Setup drift monitoring
drift_detector = TemporalDriftDetector(psi_threshold=0.2)
drift_detector.fit_reference(X_train, y_train, y_pred_train, timestamps_train)

# Monitor new data
drift_results = drift_detector.detect_drift(X_new, y_new, y_pred_new, timestamps_new)

if drift_results['overall_drift_detected']:
    print(f"Drift detected! Recommendation: {drift_results['recommendation']}")
    if drift_results['recommendation'] == 'retrain_immediately':
        # Trigger model retraining
        model.fit(X_recent, y_recent, amounts_recent)
```

## Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
git clone https://github.com/AlexShrike/fraudboost.git
cd fraudboost

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or venv\Scripts\activate  # Windows

# Install in development mode
pip install -e .[dev]

# Install pre-commit hooks
pre-commit install

# Run tests
python -m pytest tests/ -v
```

## Citation

If you use FraudBoost in your research, please cite:

```bibtex
@software{fraudboost2026,
  author = {Alex Shrike},
  title = {FraudBoost: Value-Weighted Gradient Boosting for Fraud Detection},
  url = {https://github.com/AlexShrike/fraudboost},
  year = {2026}
}
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Support

- **Documentation**: [GitHub README](https://github.com/AlexShrike/fraudboost#readme)
- **Issues**: [GitHub Issues](https://github.com/AlexShrike/fraudboost/issues)
- **Discussions**: [GitHub Discussions](https://github.com/AlexShrike/fraudboost/discussions)

---

**Built for fraud fighters, by fraud fighters** üõ°Ô∏è
