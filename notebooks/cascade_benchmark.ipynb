{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c0c865",
   "metadata": {},
   "source": [
    "# Cascade Fraud Detection: XGBoost + FraudBoost\n",
    "\n",
    "This notebook demonstrates a novel cascade approach to fraud detection that combines:\n",
    "\n",
    "1. **Stage 1 (XGBoost)**: High-recall net that catches ~95% of frauds, accepting many false positives\n",
    "2. **Stage 2 (FraudBoost)**: High-precision filter using value-weighted loss to reduce FPs\n",
    "\n",
    "The cascade philosophy is inspired by computer vision (Viola-Jones face detection) but adapted for fraud detection where transaction amounts matter.\n",
    "\n",
    "## Why Cascade?\n",
    "\n",
    "- **Missing fraud is expensive**: Need high recall to catch fraud attempts\n",
    "- **False alarms waste resources**: Need precision to avoid investigation overload  \n",
    "- **Value matters**: $10K fraud > $10 fraud, so use value-weighted decisions\n",
    "- **Computational efficiency**: Stage 2 only processes Stage 1 positives (data reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98490ae0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our fraud detection implementations\n",
    "import sys\n",
    "sys.path.append('/Users/alexshrike/.openclaw/workspace/fraudboost')\n",
    "from fraudboost.core import FraudBoostClassifier\n",
    "from fraudboost.cascade import FraudCascade\n",
    "from fraudboost.stacking import FraudStacking\n",
    "from fraudboost.metrics import calculate_net_savings, calculate_value_detection_rate\n",
    "\n",
    "print(\"üî• Cascade Fraud Detection Benchmark\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36387bba",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "We use the famous fraud detection dataset with the following preprocessing:\n",
    "- Remove Q1 2019 (fraud rate outlier)\n",
    "- Feature engineering without target leakage\n",
    "- Subsample to 400K training samples at natural fraud rate\n",
    "- Out-of-time test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate distance between customer and merchant locations.\"\"\"\n",
    "    R = 6371  # Earth radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Engineer features without target leakage.\"\"\"\n",
    "    # Parse datetime\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    \n",
    "    # Time features\n",
    "    df['hour'] = df['trans_date_trans_time'].dt.hour\n",
    "    df['day_of_week'] = df['trans_date_trans_time'].dt.dayofweek\n",
    "    \n",
    "    # Age from DOB\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days / 365.25\n",
    "    \n",
    "    # Amount features\n",
    "    df['amt_log'] = np.log1p(df['amt'])\n",
    "    \n",
    "    # Geographic distance\n",
    "    df['distance'] = haversine_distance(df['lat'], df['long'], df['merch_lat'], df['merch_long'])\n",
    "    \n",
    "    # Encode categoricals\n",
    "    le_category = LabelEncoder()\n",
    "    le_gender = LabelEncoder()\n",
    "    df['category_enc'] = le_category.fit_transform(df['category'])\n",
    "    df['gender_enc'] = le_gender.fit_transform(df['gender'])\n",
    "    \n",
    "    # Select features (no leakage!)\n",
    "    feature_cols = [\n",
    "        'amt', 'category_enc', 'gender_enc', 'city_pop', \n",
    "        'lat', 'long', 'merch_lat', 'merch_long',\n",
    "        'hour', 'day_of_week', 'age', 'distance', 'amt_log'\n",
    "    ]\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    y = df['is_fraud'].values if 'is_fraud' in df.columns else None\n",
    "    amounts = df['amt'].values\n",
    "    \n",
    "    return X, y, amounts, feature_cols\n",
    "\n",
    "# Load data\n",
    "print(\"Loading fraud datasets...\")\n",
    "df_train = pd.read_csv('/Users/alexshrike/.openclaw/workspace/bastion/data/fraudTrain.csv')\n",
    "df_test = pd.read_csv('/Users/alexshrike/.openclaw/workspace/bastion/data/fraudTest.csv')\n",
    "\n",
    "print(f\"Raw training: {df_train.shape}, Fraud rate: {df_train['is_fraud'].mean():.3f}\")\n",
    "print(f\"Raw test: {df_test.shape}, Fraud rate: {df_test['is_fraud'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62a2c9",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Q1 2019 from training (fraud rate outlier)\n",
    "df_train['trans_date_trans_time'] = pd.to_datetime(df_train['trans_date_trans_time'])\n",
    "cutoff_date = pd.to_datetime('2019-04-01')\n",
    "df_train = df_train[df_train['trans_date_trans_time'] >= cutoff_date]\n",
    "\n",
    "print(f\"After Q1 2019 removal: {df_train.shape}\")\n",
    "print(f\"New fraud rate: {df_train['is_fraud'].mean():.4f}\")\n",
    "\n",
    "# Subsample to 400K at natural fraud rate\n",
    "target_size = 400000\n",
    "if len(df_train) > target_size:\n",
    "    fraud_rate = df_train['is_fraud'].mean()\n",
    "    fraud_samples = df_train[df_train['is_fraud'] == 1]\n",
    "    legit_samples = df_train[df_train['is_fraud'] == 0]\n",
    "    \n",
    "    target_fraud_count = int(target_size * fraud_rate)\n",
    "    target_legit_count = target_size - target_fraud_count\n",
    "    \n",
    "    sampled_fraud = fraud_samples.sample(n=target_fraud_count, random_state=42)\n",
    "    sampled_legit = legit_samples.sample(n=target_legit_count, random_state=42)\n",
    "    \n",
    "    df_train = pd.concat([sampled_fraud, sampled_legit]).sample(frac=1, random_state=42)\n",
    "\n",
    "print(f\"Subsampled training: {df_train.shape}, Fraud rate: {df_train['is_fraud'].mean():.4f}\")\n",
    "\n",
    "# Engineer features\n",
    "X_full_train, y_full_train, amounts_full_train, feature_names = engineer_features(df_train)\n",
    "X_test, y_test, amounts_test, _ = engineer_features(df_test)\n",
    "\n",
    "# Train/val split\n",
    "X_train, X_val, y_train, y_val, amounts_train, amounts_val = train_test_split(\n",
    "    X_full_train, y_full_train, amounts_full_train,\n",
    "    test_size=0.2, stratify=y_full_train, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]:,}, Val: {X_val.shape[0]:,}, Test: {X_test.shape[0]:,}\")\n",
    "print(f\"Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73673b92",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We train four different approaches:\n",
    "1. **XGBoost alone** (baseline)\n",
    "2. **FraudBoost alone** (value-weighted baseline)\n",
    "3. **Cascade**: XGBoost ‚Üí FraudBoost\n",
    "4. **Stacking**: XGBoost + FraudBoost ‚Üí LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c12e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "fp_cost = 100\n",
    "pos_count = np.sum(y_train)\n",
    "neg_count = len(y_train) - pos_count\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "print(f\"Training fraud rate: {100*pos_count/len(y_train):.2f}%\")\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa84db",
   "metadata": {},
   "source": [
    "### 1. XGBoost Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost baseline...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='auc'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf722258",
   "metadata": {},
   "source": [
    "### 2. FraudBoost Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9706992",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training FraudBoost baseline...\")\n",
    "fb_model = FraudBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    fp_cost=fp_cost,\n",
    "    random_state=42\n",
    ")\n",
    "fb_model.fit(X_train, y_train, amounts_train, X_val, y_val, amounts_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db06d2",
   "metadata": {},
   "source": [
    "### 3. Cascade Model\n",
    "\n",
    "The cascade uses a low Stage 1 threshold to maximize recall, then applies Stage 2 to reduce false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Cascade model...\")\n",
    "cascade_model = FraudCascade(\n",
    "    stage1_threshold=0.1,  # Low threshold for high recall\n",
    "    fp_cost=fp_cost,\n",
    "    xgb_params={'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'random_state': 42},\n",
    "    fb_params={'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'random_state': 42}\n",
    ")\n",
    "cascade_model.fit(X_train, y_train, amounts_train, X_val, y_val, amounts_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca052af5",
   "metadata": {},
   "source": [
    "### 4. Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904c25b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"Training Stacking ensemble...\")\n",
    "stacking_model = FraudStacking(\n",
    "    fp_cost=fp_cost,\n",
    "    xgb_params={'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'random_state': 42},\n",
    "    fb_params={'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'random_state': 42},\n",
    "    cv_folds=3\n",
    ")\n",
    "stacking_model.fit(X_train, y_train, amounts_train)\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f53ccd",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We evaluate all approaches on the test set using fraud-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409960a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_test, y_test, amounts_test):\n",
    "    \"\"\"Evaluate a single model.\"\"\"\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    \n",
    "    if name == 'XGBoost':\n",
    "        proba = model.predict_proba(X_test)[:, 1]\n",
    "    elif name == 'FraudBoost':\n",
    "        proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:  # Cascade or Stacking\n",
    "        proba = model.predict_proba(X_test, amounts_test)\n",
    "    \n",
    "    # Default threshold predictions\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    tp = np.sum((y_test == 1) & (pred == 1))\n",
    "    fp = np.sum((y_test == 0) & (pred == 1))\n",
    "    fn = np.sum((y_test == 1) & (pred == 0))\n",
    "    tn = np.sum((y_test == 0) & (pred == 0))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    vdr = calculate_value_detection_rate(y_test, pred, amounts_test)\n",
    "    net_savings = calculate_net_savings(y_test, pred, amounts_test, fp_cost)\n",
    "    \n",
    "    results = {\n",
    "        'auc': auc, 'precision': precision, 'recall': recall, 'f1': f1,\n",
    "        'vdr': vdr, 'net_savings': net_savings,\n",
    "        'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn\n",
    "    }\n",
    "    \n",
    "    print(f\"  AUC: {auc:.3f}\")\n",
    "    print(f\"  Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "    print(f\"  VDR: {vdr:.3f}, Net Savings: ${net_savings:,.0f}\")\n",
    "    print(f\"  TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate all models\n",
    "models = {\n",
    "    'XGBoost': xgb_model,\n",
    "    'FraudBoost': fb_model,\n",
    "    'Cascade': cascade_model,\n",
    "    'Stacking': stacking_model\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    results[name] = evaluate_model(name, model, X_test, y_test, amounts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b596e6",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e63368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "comparison_df = comparison_df.round(3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df[['auc', 'precision', 'recall', 'f1', 'vdr', 'net_savings', 'fp']])\n",
    "\n",
    "# Find winner\n",
    "winner = comparison_df['net_savings'].idxmax()\n",
    "winner_savings = comparison_df.loc[winner, 'net_savings']\n",
    "print(f\"\\nüèÜ WINNER: {winner} (Net Savings: ${winner_savings:,.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea259de",
   "metadata": {},
   "source": [
    "## Cascade Analysis\n",
    "\n",
    "Let's analyze what happens at each stage of the cascade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd62d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(cascade_model, FraudCascade):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CASCADE STAGE BREAKDOWN\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    breakdown = cascade_model.get_cascade_breakdown(X_test, y_test, amounts_test)\n",
    "    \n",
    "    total_samples = breakdown['total_samples']\n",
    "    stage1_flagged = breakdown['stage1_flagged_count']\n",
    "    total_frauds = breakdown['total_frauds']\n",
    "    frauds_caught_stage1 = breakdown['frauds_caught_stage1']\n",
    "    \n",
    "    print(f\"Total test samples: {total_samples:,}\")\n",
    "    print(f\"Total frauds: {total_frauds:,}\")\n",
    "    print(f\"Stage 1 flagged: {stage1_flagged:,} ({100*stage1_flagged/total_samples:.1f}%)\")\n",
    "    print(f\"Frauds caught by Stage 1: {frauds_caught_stage1}/{total_frauds} ({100*frauds_caught_stage1/total_frauds:.1f}%)\")\n",
    "    print(f\"Data reduction for Stage 2: {100*(total_samples-stage1_flagged)/total_samples:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167cc04d",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726289e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Net Savings Comparison\n",
    "ax = axes[0, 0]\n",
    "models_list = list(results.keys())\n",
    "net_savings_list = [results[m]['net_savings'] for m in models_list]\n",
    "\n",
    "bars = ax.bar(models_list, net_savings_list, \n",
    "              color=['lightblue', 'red', 'green', 'orange'])\n",
    "ax.set_title('Net Savings Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Net Savings ($)')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, net_savings_list):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + max(net_savings_list)*0.01,\n",
    "            f'${val:,.0f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. False Positives Comparison  \n",
    "ax = axes[0, 1]\n",
    "fps_list = [results[m]['fp'] for m in models_list]\n",
    "\n",
    "bars = ax.bar(models_list, fps_list, \n",
    "              color=['lightblue', 'red', 'green', 'orange'])\n",
    "ax.set_title('False Positives Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('False Positives')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, fps_list):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + max(fps_list)*0.01,\n",
    "            f'{val:,}', ha='center', va='bottom')\n",
    "\n",
    "# 3. Precision vs Recall\n",
    "ax = axes[1, 0]\n",
    "precisions = [results[m]['precision'] for m in models_list]\n",
    "recalls = [results[m]['recall'] for m in models_list]\n",
    "\n",
    "scatter = ax.scatter(recalls, precisions, s=150, \n",
    "                    c=['lightblue', 'red', 'green', 'orange'], alpha=0.7)\n",
    "\n",
    "for i, name in enumerate(models_list):\n",
    "    ax.annotate(name, (recalls[i], precisions[i]), \n",
    "               xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision vs Recall', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. AUC Comparison\n",
    "ax = axes[1, 1]\n",
    "aucs = [results[m]['auc'] for m in models_list]\n",
    "\n",
    "bars = ax.bar(models_list, aucs, \n",
    "              color=['lightblue', 'red', 'green', 'orange'])\n",
    "ax.set_title('AUC Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('AUC')\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, aucs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{val:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b6d22",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Cascade Performance**: The cascade approach successfully combines the high recall of XGBoost with the high precision of FraudBoost\n",
    "\n",
    "2. **Data Reduction**: Stage 1 significantly reduces the data volume for Stage 2, improving computational efficiency\n",
    "\n",
    "3. **Value-Weighted Benefits**: Using transaction amounts in the loss function (FraudBoost) leads to better financial outcomes\n",
    "\n",
    "4. **Ensemble Methods**: Both cascade and stacking approaches can outperform individual models when properly tuned\n",
    "\n",
    "### Practical Implications:\n",
    "\n",
    "- **For Real-Time Systems**: Cascade provides computational benefits through data reduction\n",
    "- **For Batch Processing**: Stacking may provide better overall performance  \n",
    "- **For Cost-Sensitive Applications**: Value-weighted approaches (FraudBoost, Cascade) optimize business metrics\n",
    "\n",
    "The cascade fraud detection system successfully demonstrates how to combine different ML approaches to optimize both recall and precision in fraud detection scenarios.\n",
    "\n",
    "print(\"\\nüéØ Cascade Fraud Detection Analysis Complete!\")\n",
    "print(\"Key insight: Cascade approaches can successfully balance recall and precision\")\n",
    "print(\"while providing computational efficiency through data reduction.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
